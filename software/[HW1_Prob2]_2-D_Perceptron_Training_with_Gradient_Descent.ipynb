{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c938710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2 is [(1.0, 2.1, 3.0), (2.0, 3.5, 6.0)]\n",
      "Net(\n",
      "  (fc1): Linear(in_features=2, out_features=1, bias=False)\n",
      ")\n",
      "[Parameter containing:\n",
      "tensor([[0.0450, 0.4584]], requires_grad=True)]\n",
      "Number of batches: 2\n",
      "Epoch 0 - loss: 26.2028865814209\n",
      "Epoch 0 - loss: 79.55927276611328\n",
      "Epoch 1 - loss: 6.550475597381592\n",
      "Epoch 1 - loss: 146.4219970703125\n",
      "Epoch 2 - loss: 4.2151265144348145\n",
      "Epoch 2 - loss: 106.43270111083984\n",
      "Epoch 3 - loss: 1.695000171661377\n",
      "Epoch 3 - loss: 65.41062927246094\n",
      "Epoch 4 - loss: 1.6572219133377075\n",
      "Epoch 4 - loss: 34.15116500854492\n",
      "Epoch 5 - loss: 0.9694628715515137\n",
      "Epoch 5 - loss: 19.711990356445312\n",
      "Epoch 6 - loss: 0.959291934967041\n",
      "Epoch 6 - loss: 10.643476486206055\n",
      "Epoch 7 - loss: 0.6773732304573059\n",
      "Epoch 7 - loss: 7.053180694580078\n",
      "Epoch 8 - loss: 0.6330312490463257\n",
      "Epoch 8 - loss: 4.504974365234375\n",
      "Epoch 9 - loss: 0.48886775970458984\n",
      "Epoch 9 - loss: 3.4874751567840576\n",
      "Epoch 10 - loss: 0.4375421106815338\n",
      "Epoch 10 - loss: 2.5906076431274414\n",
      "Epoch 11 - loss: 0.3530700206756592\n",
      "Epoch 11 - loss: 2.1592137813568115\n",
      "Epoch 12 - loss: 0.3079439401626587\n",
      "Epoch 12 - loss: 1.7308472394943237\n",
      "Epoch 13 - loss: 0.2540726661682129\n",
      "Epoch 13 - loss: 1.4702199697494507\n",
      "Epoch 14 - loss: 0.21842223405838013\n",
      "Epoch 14 - loss: 1.2153964042663574\n",
      "Epoch 15 - loss: 0.18229015171527863\n",
      "Epoch 15 - loss: 1.0338695049285889\n",
      "Epoch 16 - loss: 0.1554843634366989\n",
      "Epoch 16 - loss: 0.8652272820472717\n",
      "Epoch 17 - loss: 0.13054408133029938\n",
      "Epoch 17 - loss: 0.7345921993255615\n",
      "Epoch 18 - loss: 0.11087734252214432\n",
      "Epoch 18 - loss: 0.6179772615432739\n",
      "Epoch 19 - loss: 0.0933869406580925\n",
      "Epoch 19 - loss: 0.5237372517585754\n",
      "Epoch 20 - loss: 0.07913877815008163\n",
      "Epoch 20 - loss: 0.44164466857910156\n",
      "Epoch 21 - loss: 0.06676614284515381\n",
      "Epoch 21 - loss: 0.373860239982605\n",
      "Epoch 22 - loss: 0.05651143193244934\n",
      "Epoch 22 - loss: 0.31562289595603943\n",
      "Epoch 23 - loss: 0.04771864414215088\n",
      "Epoch 23 - loss: 0.2669990360736847\n",
      "Epoch 24 - loss: 0.040363553911447525\n",
      "Epoch 24 - loss: 0.22553898394107819\n",
      "Epoch 25 - loss: 0.03409929201006889\n",
      "Epoch 25 - loss: 0.1907210499048233\n",
      "Epoch 26 - loss: 0.028833428397774696\n",
      "Epoch 26 - loss: 0.16115373373031616\n",
      "Epoch 27 - loss: 0.02436465583741665\n",
      "Epoch 27 - loss: 0.13624794781208038\n",
      "Epoch 28 - loss: 0.020598508417606354\n",
      "Epoch 28 - loss: 0.11514323949813843\n",
      "Epoch 29 - loss: 0.0174082200974226\n",
      "Epoch 29 - loss: 0.0973375216126442\n",
      "Epoch 30 - loss: 0.014716017059981823\n",
      "Epoch 30 - loss: 0.08226688951253891\n",
      "Epoch 31 - loss: 0.012437686324119568\n",
      "Epoch 31 - loss: 0.06954073905944824\n",
      "Epoch 32 - loss: 0.010513548739254475\n",
      "Epoch 32 - loss: 0.05877627804875374\n",
      "Epoch 33 - loss: 0.008886201307177544\n",
      "Epoch 33 - loss: 0.04968230426311493\n",
      "Epoch 34 - loss: 0.007511330768465996\n",
      "Epoch 34 - loss: 0.04199303686618805\n",
      "Epoch 35 - loss: 0.006348814815282822\n",
      "Epoch 35 - loss: 0.035495173186063766\n",
      "Epoch 36 - loss: 0.005366433411836624\n",
      "Epoch 36 - loss: 0.030001988634467125\n",
      "Epoch 37 - loss: 0.0045359148643910885\n",
      "Epoch 37 - loss: 0.02535945177078247\n",
      "Epoch 38 - loss: 0.003834034316241741\n",
      "Epoch 38 - loss: 0.021434927359223366\n",
      "Epoch 39 - loss: 0.0032407294493168592\n",
      "Epoch 39 - loss: 0.01811787858605385\n",
      "Epoch 40 - loss: 0.0027392113115638494\n",
      "Epoch 40 - loss: 0.015314072370529175\n",
      "Epoch 41 - loss: 0.0023152728099375963\n",
      "Epoch 41 - loss: 0.012944325804710388\n",
      "Epoch 42 - loss: 0.00195700372569263\n",
      "Epoch 42 - loss: 0.010941232554614544\n",
      "Epoch 43 - loss: 0.0016541685909032822\n",
      "Epoch 43 - loss: 0.009248037822544575\n",
      "Epoch 44 - loss: 0.001398197840899229\n",
      "Epoch 44 - loss: 0.007816861383616924\n",
      "Epoch 45 - loss: 0.0011817794293165207\n",
      "Epoch 45 - loss: 0.006607165094465017\n",
      "Epoch 46 - loss: 0.0009989235550165176\n",
      "Epoch 46 - loss: 0.005584757775068283\n",
      "Epoch 47 - loss: 0.0008443589322268963\n",
      "Epoch 47 - loss: 0.004720540251582861\n",
      "Epoch 48 - loss: 0.0007136711501516402\n",
      "Epoch 48 - loss: 0.003990038763731718\n",
      "Epoch 49 - loss: 0.0006032343953847885\n",
      "Epoch 49 - loss: 0.0033725465182214975\n",
      "Epoch 50 - loss: 0.0005098764668218791\n",
      "Epoch 50 - loss: 0.0028506687376648188\n",
      "Epoch 51 - loss: 0.00043099114554934204\n",
      "Epoch 51 - loss: 0.002409510314464569\n",
      "Epoch 52 - loss: 0.0003642857482191175\n",
      "Epoch 52 - loss: 0.0020366774406284094\n",
      "Epoch 53 - loss: 0.0003079257148783654\n",
      "Epoch 53 - loss: 0.0017214692197740078\n",
      "Epoch 54 - loss: 0.00026026988052763045\n",
      "Epoch 54 - loss: 0.0014551096828654408\n",
      "Epoch 55 - loss: 0.00021998929150868207\n",
      "Epoch 55 - loss: 0.001229912624694407\n",
      "Epoch 56 - loss: 0.0001859524054452777\n",
      "Epoch 56 - loss: 0.0010395987192168832\n",
      "Epoch 57 - loss: 0.00015717747737653553\n",
      "Epoch 57 - loss: 0.0008787180413492024\n",
      "Epoch 58 - loss: 0.00013285236491356045\n",
      "Epoch 58 - loss: 0.0007427383097819984\n",
      "Epoch 59 - loss: 0.00011229809751966968\n",
      "Epoch 59 - loss: 0.0006278049550019205\n",
      "Epoch 60 - loss: 9.492167737334967e-05\n",
      "Epoch 60 - loss: 0.0005306767416186631\n",
      "Epoch 61 - loss: 8.022820838959888e-05\n",
      "Epoch 61 - loss: 0.00044853854342363775\n",
      "Epoch 62 - loss: 6.780905096093193e-05\n",
      "Epoch 62 - loss: 0.00037912651896476746\n",
      "Epoch 63 - loss: 5.731999408453703e-05\n",
      "Epoch 63 - loss: 0.00032045305124484\n",
      "Epoch 64 - loss: 4.844965224037878e-05\n",
      "Epoch 64 - loss: 0.00027087112539447844\n",
      "Epoch 65 - loss: 4.095339681953192e-05\n",
      "Epoch 65 - loss: 0.00022895517759025097\n",
      "Epoch 66 - loss: 3.46121632901486e-05\n",
      "Epoch 66 - loss: 0.00019351881928741932\n",
      "Epoch 67 - loss: 2.9254557375679724e-05\n",
      "Epoch 67 - loss: 0.00016357236017938703\n",
      "Epoch 68 - loss: 2.4731207304284908e-05\n",
      "Epoch 68 - loss: 0.00013825860514771193\n",
      "Epoch 69 - loss: 2.090227098960895e-05\n",
      "Epoch 69 - loss: 0.0001168597154901363\n",
      "Epoch 70 - loss: 1.766685272741597e-05\n",
      "Epoch 70 - loss: 9.877980482997373e-05\n",
      "Epoch 71 - loss: 1.4933473721612245e-05\n",
      "Epoch 71 - loss: 8.348785195266828e-05\n",
      "Epoch 72 - loss: 1.2624775081349071e-05\n",
      "Epoch 72 - loss: 7.057091715978459e-05\n",
      "Epoch 73 - loss: 1.0669697985576931e-05\n",
      "Epoch 73 - loss: 5.964880983810872e-05\n",
      "Epoch 74 - loss: 9.017850061354693e-06\n",
      "Epoch 74 - loss: 5.042187331127934e-05\n",
      "Epoch 75 - loss: 7.622029443155043e-06\n",
      "Epoch 75 - loss: 4.26196311309468e-05\n",
      "Epoch 76 - loss: 6.441983259719564e-06\n",
      "Epoch 76 - loss: 3.60235535481479e-05\n",
      "Epoch 77 - loss: 5.4464057939185295e-06\n",
      "Epoch 77 - loss: 3.0447516110143624e-05\n",
      "Epoch 78 - loss: 4.602984063240001e-06\n",
      "Epoch 78 - loss: 2.5741779609234072e-05\n",
      "Epoch 79 - loss: 3.8918096834095195e-06\n",
      "Epoch 79 - loss: 2.1751397071057e-05\n",
      "Epoch 80 - loss: 3.2887292036321014e-06\n",
      "Epoch 80 - loss: 1.8389077013125643e-05\n",
      "Epoch 81 - loss: 2.7804792352981167e-06\n",
      "Epoch 81 - loss: 1.5542314940830693e-05\n",
      "Epoch 82 - loss: 2.349652277189307e-06\n",
      "Epoch 82 - loss: 1.314019664278021e-05\n",
      "Epoch 83 - loss: 1.9866674847435206e-06\n",
      "Epoch 83 - loss: 1.1102514690719545e-05\n",
      "Epoch 84 - loss: 1.6796593627077527e-06\n",
      "Epoch 84 - loss: 9.386259080201853e-06\n",
      "Epoch 85 - loss: 1.420074909219693e-06\n",
      "Epoch 85 - loss: 7.93314666225342e-06\n",
      "Epoch 86 - loss: 1.199327130052552e-06\n",
      "Epoch 86 - loss: 6.7059663706459105e-06\n",
      "Epoch 87 - loss: 1.0140450967810466e-06\n",
      "Epoch 87 - loss: 5.667573077516863e-06\n",
      "Epoch 88 - loss: 8.568492262384098e-07\n",
      "Epoch 88 - loss: 4.792131676367717e-06\n",
      "Epoch 89 - loss: 7.247981557156891e-07\n",
      "Epoch 89 - loss: 4.049203653266886e-06\n",
      "Epoch 90 - loss: 6.122189120105759e-07\n",
      "Epoch 90 - loss: 3.423326234042179e-06\n",
      "Epoch 91 - loss: 5.175834871806728e-07\n",
      "Epoch 91 - loss: 2.8910619676025817e-06\n",
      "Epoch 92 - loss: 4.377269249289384e-07\n",
      "Epoch 92 - loss: 2.445499376335647e-06\n",
      "Epoch 93 - loss: 3.698982311561849e-07\n",
      "Epoch 93 - loss: 2.06825575332914e-06\n",
      "Epoch 94 - loss: 3.126854437596194e-07\n",
      "Epoch 94 - loss: 1.7466132931076572e-06\n",
      "Epoch 95 - loss: 2.64025118212885e-07\n",
      "Epoch 95 - loss: 1.477256432735885e-06\n",
      "Epoch 96 - loss: 2.2300713453660137e-07\n",
      "Epoch 96 - loss: 1.248628564098908e-06\n",
      "Epoch 97 - loss: 1.882589089063913e-07\n",
      "Epoch 97 - loss: 1.0548980071689584e-06\n",
      "Epoch 98 - loss: 1.5971220079791237e-07\n",
      "Epoch 98 - loss: 8.922385745790962e-07\n",
      "Epoch 99 - loss: 1.3468866200128105e-07\n",
      "Epoch 99 - loss: 7.536376074313011e-07\n",
      "Epoch 100 - loss: 1.1388215170882177e-07\n",
      "Epoch 100 - loss: 6.362282078953285e-07\n",
      "Epoch 101 - loss: 9.619552798767472e-08\n",
      "Epoch 101 - loss: 5.386888801695022e-07\n",
      "Epoch 102 - loss: 8.133165607659976e-08\n",
      "Epoch 102 - loss: 4.544602631995076e-07\n",
      "Epoch 103 - loss: 6.890096670986168e-08\n",
      "Epoch 103 - loss: 3.8471625884994864e-07\n",
      "Epoch 104 - loss: 5.8023204019264085e-08\n",
      "Epoch 104 - loss: 3.2563971785748436e-07\n",
      "Epoch 105 - loss: 4.929440322598566e-08\n",
      "Epoch 105 - loss: 2.749451084582688e-07\n",
      "Epoch 106 - loss: 4.1485463952994905e-08\n",
      "Epoch 106 - loss: 2.3241652513661393e-07\n",
      "Epoch 107 - loss: 3.523966185525751e-08\n",
      "Epoch 107 - loss: 1.9603400858159148e-07\n",
      "Epoch 108 - loss: 2.9697901027248008e-08\n",
      "Epoch 108 - loss: 1.6560322535497107e-07\n",
      "Epoch 109 - loss: 2.5105464374064468e-08\n",
      "Epoch 109 - loss: 1.4021286176557624e-07\n",
      "Epoch 110 - loss: 2.130551912671308e-08\n",
      "Epoch 110 - loss: 1.184516804642044e-07\n",
      "Epoch 111 - loss: 1.7939195018357168e-08\n",
      "Epoch 111 - loss: 1.0014415607884075e-07\n",
      "Epoch 112 - loss: 1.5170693146160374e-08\n",
      "Epoch 112 - loss: 8.463575795758516e-08\n",
      "Epoch 113 - loss: 1.2805458027287386e-08\n",
      "Epoch 113 - loss: 7.154449122026563e-08\n",
      "Epoch 114 - loss: 1.082495781901116e-08\n",
      "Epoch 114 - loss: 6.062115431859638e-08\n",
      "Epoch 115 - loss: 9.170435077976435e-09\n",
      "Epoch 115 - loss: 5.1170598425187563e-08\n",
      "Epoch 116 - loss: 7.740349694529414e-09\n",
      "Epoch 116 - loss: 4.32510205428116e-08\n",
      "Epoch 117 - loss: 6.559048415510915e-09\n",
      "Epoch 117 - loss: 3.654167812783271e-08\n",
      "Epoch 118 - loss: 5.512693856957185e-09\n",
      "Epoch 118 - loss: 3.0784576665610075e-08\n",
      "Epoch 119 - loss: 4.66322580550127e-09\n",
      "Epoch 119 - loss: 2.6171619538217783e-08\n",
      "Epoch 120 - loss: 3.997001840616576e-09\n",
      "Epoch 120 - loss: 2.2012500267010182e-08\n",
      "Epoch 121 - loss: 3.3076048566726968e-09\n",
      "Epoch 121 - loss: 1.874347255181874e-08\n",
      "Epoch 122 - loss: 2.8313518196654286e-09\n",
      "Epoch 122 - loss: 1.5795498242709982e-08\n",
      "Epoch 123 - loss: 2.377494867644714e-09\n",
      "Epoch 123 - loss: 1.3372603469008482e-08\n",
      "Epoch 124 - loss: 2.0373629450176622e-09\n",
      "Epoch 124 - loss: 1.1336548588758433e-08\n",
      "Epoch 125 - loss: 1.7008497943393763e-09\n",
      "Epoch 125 - loss: 9.556060831528157e-09\n",
      "Epoch 126 - loss: 1.4468733988692861e-09\n",
      "Epoch 126 - loss: 8.145434549078345e-09\n",
      "Epoch 127 - loss: 1.2338811083978385e-09\n",
      "Epoch 127 - loss: 6.878508429508656e-09\n",
      "Epoch 128 - loss: 1.0519821680432528e-09\n",
      "Epoch 128 - loss: 5.744671849328142e-09\n",
      "Epoch 129 - loss: 8.660852590658408e-10\n",
      "Epoch 129 - loss: 4.791824270711231e-09\n",
      "Epoch 130 - loss: 7.399686974274289e-10\n",
      "Epoch 130 - loss: 4.137897580136496e-09\n",
      "Epoch 131 - loss: 6.108962224971037e-10\n",
      "Epoch 131 - loss: 3.4694191963779986e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132 - loss: 5.292690730129834e-10\n",
      "Epoch 132 - loss: 2.9485818231478333e-09\n",
      "Epoch 133 - loss: 4.676508069678675e-10\n",
      "Epoch 133 - loss: 2.464427550918913e-09\n",
      "Epoch 134 - loss: 3.6561687011271715e-10\n",
      "Epoch 134 - loss: 2.0627339836210012e-09\n",
      "Epoch 135 - loss: 3.1038402403815724e-10\n",
      "Epoch 135 - loss: 1.7777589400580496e-09\n",
      "Epoch 136 - loss: 2.6981675227411017e-10\n",
      "Epoch 136 - loss: 1.4797478797845542e-09\n",
      "Epoch 137 - loss: 2.326032699784264e-10\n",
      "Epoch 137 - loss: 1.277536854082939e-09\n",
      "Epoch 138 - loss: 2.0010777757040188e-10\n",
      "Epoch 138 - loss: 1.0589550347717136e-09\n",
      "Epoch 139 - loss: 1.6772598587788679e-10\n",
      "Epoch 139 - loss: 8.794813766144216e-10\n",
      "Epoch 140 - loss: 1.3591261449619196e-10\n",
      "Epoch 140 - loss: 7.397223389382646e-10\n",
      "Epoch 141 - loss: 1.174764036049325e-10\n",
      "Epoch 141 - loss: 6.384652806445956e-10\n",
      "Epoch 142 - loss: 9.29389898374211e-11\n",
      "Epoch 142 - loss: 5.629772203974426e-10\n",
      "Epoch 143 - loss: 8.314297167411056e-11\n",
      "Epoch 143 - loss: 4.5232204093359485e-10\n",
      "Epoch 144 - loss: 6.374042266221736e-11\n",
      "Epoch 144 - loss: 3.895669065112628e-10\n",
      "Epoch 145 - loss: 6.572994232234564e-11\n",
      "Epoch 145 - loss: 3.219611244276166e-10\n",
      "Epoch 146 - loss: 4.6251596302893105e-11\n",
      "Epoch 146 - loss: 2.8164018339715824e-10\n",
      "Epoch 147 - loss: 4.587263902067207e-11\n",
      "Epoch 147 - loss: 2.3313381780631914e-10\n",
      "Epoch 148 - loss: 3.168073095527468e-11\n",
      "Epoch 148 - loss: 2.088806211331118e-10\n",
      "Epoch 149 - loss: 2.5844807197139552e-11\n",
      "Epoch 149 - loss: 1.6795335955333002e-10\n",
      "when x = tensor([1.0000, 2.1000]), y = tensor([3.0000], grad_fn=<SqueezeBackward4>)\n",
      "when x = tensor([2.0000, 3.5000]), y = tensor([6.0000], grad_fn=<SqueezeBackward4>)\n",
      "when x = tensor([3., 3.]), y = tensor([9.0000], grad_fn=<SqueezeBackward4>)\n",
      "when x = tensor([4.0000, 2.1000]), y = tensor([12.0000], grad_fn=<SqueezeBackward4>)\n",
      "when x = tensor([5.0000, 7.2000]), y = tensor([15.0000], grad_fn=<SqueezeBackward4>)\n",
      "when x = tensor([ 6.0000, 10.1000]), y = tensor([18.0000], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "### Training with manually updating W with \"Backward\" ###\n",
    "\n",
    "import torch\n",
    "#from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "data = [(1.0,2.1,3.0), (2.0, 3.5, 6.0), (3.0, 3.0, 9.0), (4.0, 2.1, 12.0), (5.0, 7.2, 15.0), (6.0, 10.1, 18.0)]\n",
    "t2 = data[0:2]\n",
    "print(f't2 is {t2}')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,1,bias=False)  # in dim, out dim\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "print(net)\n",
    "print(list(net.parameters()))\n",
    "\n",
    "#input = torch.randn(1)\n",
    "#out = net(input)\n",
    "\n",
    "#def criterion(out, label):\n",
    "#    return (label - out)**2\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.005)\n",
    "\n",
    "batch_size = 3  #batch size = 3\n",
    "batch = int(len(data)/batch_size) #number of batches\n",
    "\n",
    "print(f'Number of batches: {batch}')\n",
    "\n",
    "for epoch in range(150): # 0 - 149\n",
    "    for b in range(batch): # 0 - 1\n",
    "        \n",
    "        #slicing data per batch\n",
    "        current_data = data[b*batch_size:(b+1)*batch_size]\n",
    "        \n",
    "        #Creating separate lists for input and output\n",
    "        X_3 = []\n",
    "        Y_3 = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            X_3.append([current_data[i][0],current_data[i][1]])\n",
    "            Y_3.append([current_data[i][2]])\n",
    "        #print(f'X_3 is {X_3} and Y_3 is {Y_3}')   \n",
    "        \n",
    "        #Converting list to tensors\n",
    "        X, Y = torch.FloatTensor(X_3), torch.FloatTensor(Y_3)\n",
    "        #print(f'X is {X} and Y is {Y}')\n",
    "        \n",
    "        #Optimiser and Weight update\n",
    "        optimizer.zero_grad()   \n",
    "        outputs = net(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()    ## This line is equivalent to \"W = W - lr* W.grad\"\n",
    "        ##should we multiply loss with the impact factor i.e. batch size?\n",
    "        \n",
    "        print(\"Epoch {} - loss: {}\".format(epoch, loss))\n",
    "\n",
    "### Test the trained network ###            \n",
    "for i, current_data in enumerate(data):\n",
    "    X0, X1, Y = current_data\n",
    "    X, Y = torch.FloatTensor([X0,X1]), torch.FloatTensor([Y])  \n",
    "    out = net(torch.FloatTensor(X))  \n",
    "    print(\"when x = {}, y = {}\".format(X, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c457dea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
